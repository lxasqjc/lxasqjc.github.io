

[
  
  
    
    
      {
        "title": "Interactive Visualization of Optimization Algorithms in Deep Learning",
        "excerpt": "Optimization on non convex functions in high dimensional spaces, like those encountered in deep learning, can be hard to visualize. However, we can learn a lot from visualizing optimization paths on simple 2d non convex functions.\n\nClick anywhere on the function contour to start a minimization.\n\n\n\n\nYou can toggle the different algorithms by clicking the circles in the lower bar. The code is available here.\n\n\n\nObservations\n\nThe above function is given by\n\n[f(x, y) =  x^2 + y^2 - a e^{-\\frac{(x - 1)^2 + y^2}{c}} - b e^{-\\frac{(x + 1)^2 + y^2}{d}}]\n\nIt is basically a quadratic “bowl” with two gaussians creating minima at (1, 0) and (-1, 0) respectively. The size of these minima is controlled by the \\(a\\) and \\(b\\) parameters.\n\nDifferent minima\n\nStarting from the same point, different algorithms will converge to different minima. Often, SGD and SGD with momentum will converge to the poorer minimum (the one on the right) while RMSProp and Adam will converge to the global minimum. For this particular function, Adam is the algorithm that converges to the global minimum from the most initializations.\n\n\nOnly Adam (in green) converges to the global minimum.\n\nThe effects of momentum\n\nAugmenting SGD with momentum has many advantages and often works better than the other standard algorithms for an appropriately chosen learning rate (check out this paper for more details). However, with the wrong learning rate, SGD with momentum can overshoot minima and this often leads to a spiraling pattern around the minimum.\n\n\nSGD with momentum spiraling towards the minimum.\n\nStandard SGD does not get you far\n\nSGD without momentum consistently performs the worst. The learning rate for SGD on the visualization is set to be artificially high (an order of magnitude higher than the other algorithms) in order for the optimization to converge in a reasonable amount of time.\n\n\n\nClassic optimization test functions\n\nThere are many famous test functions for optimization which are useful for testing convergence, precision, robustness and performance of optimization algorithms. They also exhibit interesting behaviour which does not appear in the above function.\n\nRastrigin\n\n\n  The visualization for this function can be found here\n\n\nA Rastrigin function is a quadratic bowl overlayed with a grid of sine bumps creating a large number of local minima.\n\n\nSGD with momentum reaches the global optimum while all other algorithms get stuck in the same local minimum.\n\nIn this example, SGD with momentum outperforms all other algorithms using the default parameter settings. The speed built up from the momentum allows it to power through the sine bumps and converge to the global minimum when other algorithms don’t. Of course, this would not necessarily be the case if the sine bumps had been scaled or spaced differently. Indeed, on the first function in this post, Adam performed the best while SGD with momentum performs the best on the Rastrigin function. This shows that there is no single algorithm that will perform the best on all functions, even in simple 2D cases.\n\nRosenbrock\n\n\n  The visualization for this function can be found here\n\n\nThe Rosenbrock function has a single global minimum inside a parabolic shaped valley. Most algorithms rapidly converge to this valley, but it is typically difficult to converge to the global minimum within this valley.\n\n\nAll algorithms find the global minimum but through very different paths\n\nWhile all algorithms converge to the optimum, the adaptive and non adaptive optimization algorithms approach the minimum through different paths. In higher dimensional problems, like in deep learning, different optimization algorithms will likely explore very different areas of parameter space.\n\n\n\nConclusion\n\nOptimization algorithms can exhibit interesting behaviour, even on simple 2d functions. Of course, there are also many phenomena which we cannot hope to visualize on simple 2d problems. Understanding and visualizing optimization in deep learning in general is an active area of research. New optimization algorithms, like Eve or YellowFin, are also being developed. It would be interesting to modify the above code to visualize these more recent algorithms, although it is unclear whether they would differ significantly from momentum SGD on these toy problems.\n\n\n\n\n\n\n\n\n\n\n\n",
        "content": "Optimization on non convex functions in high dimensional spaces, like those encountered in deep learning, can be hard to visualize. However, we can learn a lot from visualizing optimization paths on simple 2d non convex functions.\n\nClick anywhere on the function contour to start a minimization.\n\n\n\n\nYou can toggle the different algorithms by clicking the circles in the lower bar. The code is available here.\n\n\n\nObservations\n\nThe above function is given by\n\n[f(x, y) =  x^2 + y^2 - a e^{-\\frac{(x - 1)^2 + y^2}{c}} - b e^{-\\frac{(x + 1)^2 + y^2}{d}}]\n\nIt is basically a quadratic “bowl” with two gaussians creating minima at (1, 0) and (-1, 0) respectively. The size of these minima is controlled by the \\(a\\) and \\(b\\) parameters.\n\nDifferent minima\n\nStarting from the same point, different algorithms will converge to different minima. Often, SGD and SGD with momentum will converge to the poorer minimum (the one on the right) while RMSProp and Adam will converge to the global minimum. For this particular function, Adam is the algorithm that converges to the global minimum from the most initializations.\n\n\nOnly Adam (in green) converges to the global minimum.\n\nThe effects of momentum\n\nAugmenting SGD with momentum has many advantages and often works better than the other standard algorithms for an appropriately chosen learning rate (check out this paper for more details). However, with the wrong learning rate, SGD with momentum can overshoot minima and this often leads to a spiraling pattern around the minimum.\n\n\nSGD with momentum spiraling towards the minimum.\n\nStandard SGD does not get you far\n\nSGD without momentum consistently performs the worst. The learning rate for SGD on the visualization is set to be artificially high (an order of magnitude higher than the other algorithms) in order for the optimization to converge in a reasonable amount of time.\n\n\n\nClassic optimization test functions\n\nThere are many famous test functions for optimization which are useful for testing convergence, precision, robustness and performance of optimization algorithms. They also exhibit interesting behaviour which does not appear in the above function.\n\nRastrigin\n\n\n  The visualization for this function can be found here\n\n\nA Rastrigin function is a quadratic bowl overlayed with a grid of sine bumps creating a large number of local minima.\n\n\nSGD with momentum reaches the global optimum while all other algorithms get stuck in the same local minimum.\n\nIn this example, SGD with momentum outperforms all other algorithms using the default parameter settings. The speed built up from the momentum allows it to power through the sine bumps and converge to the global minimum when other algorithms don’t. Of course, this would not necessarily be the case if the sine bumps had been scaled or spaced differently. Indeed, on the first function in this post, Adam performed the best while SGD with momentum performs the best on the Rastrigin function. This shows that there is no single algorithm that will perform the best on all functions, even in simple 2D cases.\n\nRosenbrock\n\n\n  The visualization for this function can be found here\n\n\nThe Rosenbrock function has a single global minimum inside a parabolic shaped valley. Most algorithms rapidly converge to this valley, but it is typically difficult to converge to the global minimum within this valley.\n\n\nAll algorithms find the global minimum but through very different paths\n\nWhile all algorithms converge to the optimum, the adaptive and non adaptive optimization algorithms approach the minimum through different paths. In higher dimensional problems, like in deep learning, different optimization algorithms will likely explore very different areas of parameter space.\n\n\n\nConclusion\n\nOptimization algorithms can exhibit interesting behaviour, even on simple 2d functions. Of course, there are also many phenomena which we cannot hope to visualize on simple 2d problems. Understanding and visualizing optimization in deep learning in general is an active area of research. New optimization algorithms, like Eve or YellowFin, are also being developed. It would be interesting to modify the above code to visualize these more recent algorithms, although it is unclear whether they would differ significantly from momentum SGD on these toy problems.\n\n\n\n\n\n\n\n\n\n\n\n",
        "url": "/2018/01/24/optimization-visualization/"
      },
    
      {
        "title": "Passing a Chicken through an MNIST Model",
        "excerpt": "When you put a picture of a chicken through a model trained on MNIST, the model is 99.9% confident that the chicken is a 5. That’s not good.\n\nThis problem does not just relate to chickens and digits but the fact that a neural net makes very confident predictions on data that does not come from the same distribution as the training data. While this example is artificial, it is common in practice for a machine learning model to be used on data that is very different from the data it was trained on. A self-driving car, for example, may encounter an unusual environment that was never seen during training. In such cases, the system should not be overly confident but instead let the driver know that it is not able to make a meaningful prediction.1\n\n\nImages from MNIST and a chicken.\n\nDiscriminative models and unseen data\n\nWhen doing classification we are often interested in building a discriminative model \\(p(y \\vert x)\\), i.e. a model of the probability of a certain label \\(y\\) (e.g. digit type) given a datapoint \\(x\\) (e.g. an image of a digit). If we use data drawn from a distribution \\(p_{\\text{train}}(x)\\) to train a discriminative model \\(p(y \\vert x)\\), how will the trained model behave when we input an \\(x\\) that is very far from \\(p_{\\text{train}}(x)\\)? For example, if we train a model to predict digit type from an image of a digit, what happens when we put a picture of a chicken through this model?\n\n\nIn the space of images, chickens lie far away from digits. This figure shows the distribution of digits in blue (corresponding to the training distribution in our case) and where an image of a chicken would lie relative to this.\n\nChicken probabilities under an MNIST model\n\nTo explore these problems, we train a simple convolutional neural network (CNN) on MNIST which gets about 98% testing accuracy. We would then like to see what happens to the output probabilities \\(p(y \\vert x)\\) of the trained model when shown images that are completely different from digits. As an example, we pass an “MNIST-ified” chicken through the model.2\n\n\nAn MNIST-ified chicken. The CNN takes in 32 by 32 grayscale images, so we transform the image of the chicken to match this.\n\nIdeally, the outputs \\(p(y \\vert x)\\) would be approximately uniform, i.e. the probability of every class would be about 10%. This would mean that the CNN has little confidence that the chicken belongs to any of the 10 classes. However, for the above picture of a chicken, the probability of the label 5 is 99.9%.\n\n\nHistograms of expected vs actual softmax class probabilities for an image of a chicken on an MNIST model.\n\nThe model is extremely confident that this chicken is the digit 5 even though, to a human, it clearly isn’t. Even worse, it is much more confident that this chicken is a 5 than many other digits that are actually a 5.\n\n\nThe model is more confident that the image on the right is a 5 than the image on the left.\n\nFashion probabilities under an MNIST model\n\nOf course, it could be that this image of a chicken is just a fluke and high confidence predictions for data outside of \\(p_{\\text{train}}(x)\\) are rare. To test this, we use the FashionMNIST dataset which contains images of various types clothing.\n\n\nMNIST and FashionMNIST examples. The images are the same size and both contain 10 classes.\n\nThese images have nothing to do with digits, so again we would hope that the model will only make low confidence predictions. We predict \\(p(y \\vert x)\\) for 10000 images from the Fashion MNIST dataset using the trained MNIST model and measure the fraction of them which have a high confidence prediction (i.e. where the maximum probability of a certain class \\(\\max_y p(y \\vert x)\\) is very high). The results are shown below:\n\n\n  63.4% of examples have more than 99% confidence\n  74.3% of examples have more than 95% confidence\n  88.9% of examples have more than 75% confidence\n\n\nAlmost two thirds of the Fashion MNIST dataset is classified as a certain digit type with more than 99% confidence. This shows that neural nets can consistently make confident predictions about unseen data and that using the output probabilities as a measure of confidence does not make much sense, at least on data that is very far from the training data.\n\nWe can also look at how confident3 predictions on the fashion items are compared to those on correctly classified digits. To do this, we draw a vertical pink line for every FashionMNIST image and a blue line for every MNIST image. We then sort the lines by the confidence of the prediction on the corresponding image. Ideally, the resulting plot would be all pink on the left and all blue on the right (i.e all MNIST examples have higher confidence than the FashionMNIST examples under an MNIST model). The actual results are shown below.\n\n\nImages sorted by confidence. The x-axis corresponds to increasing confidence and each vertical line to an image.\n\nIdeally, all FashionMNIST images would have lower confidence and so be on the left, but this is not the case.\n\nNatural adversarial examples\n\nThe chicken and fashion images can loosely be thought of as “natural” adversarial examples. Adversarial examples are typically images from a certain class (e.g. 5) that have been imperceptibly modified to be misclassified as another class (e.g. 7) with high confidence. In the same way that adversarial examples fool the machine learning model, the chicken and fashion images “fool” the model into classifying these images into a certain class with high confidence even though they do not belong to that class (or any of the classes in our case). Machine Learning systems should not only be protected from attackers that maliciously modify images but also from naturally occurring images that are far from the training distribution.\n\nModeling the data p(x)\n\nIt seems clear that we can’t solely rely on modeling \\(p(y \\vert x)\\) when data far from \\(p_{\\text{train}}(x)\\) may be used at test time. In the real world, it is often very difficult to constrain the user only to use data drawn from \\(p_{\\text{train}}(x)\\).\n\nOne way to solve this problem is to not only model \\(p(y \\vert x)\\) but to also model \\(p_{\\text{train}}(x)\\). If we can model \\(p_{\\text{train}}(x)\\) and we get a new sample \\(x_{\\text{test}}\\), we can first check whether this sample is probable under \\(p_{\\text{train}}(x)\\). If it is, we have seen something similar before so we should go ahead and predict \\(p(y \\vert x)\\), otherwise we can reject this sample.\n\n\nSimple algorithm for returning meaningful predictions.\n\nThere are several ways of modeling p(x). In this post, we will focus on variational autoencoders (VAE) which have been quite successful at modeling distributions of images.\n\nVariational Autoencoders to model p(x)\n\nVAEs are generative models that learn a joint model \\(p(x, z)\\) of the data \\(x\\) and some latent variables \\(z\\). As the name suggests, VAEs are closely related to autoencoders. VAEs work by encoding a datapoint \\(x\\) into a distribution \\(q(z \\vert x)\\) of latent variables and then sampling a latent vector \\(z\\) from this distribution. The sample \\(z\\) is then decoded into a reconstruction of the encoded data \\(x\\). The encoder and decoder are typically neural networks.\n\n\nSketch of VAE architecture, sampling is shown with dashed lines.\n\nInterestingly, VAEs optimize a lower bound on \\(\\log p(x)\\) called the Evidence Lower Bound (ELBO).\n\n[\\log p(x) &gt;= \\text{ELBO} = - \\text{VAE loss}]\n\nSo after training a VAE on data from \\(p_{\\text{train}}\\), we can calculate the loss on a new example \\(x_{\\text{test}}\\) and obtain a lower bound on the log likelihood of that example under \\(p_{\\text{train}}\\). Of course, this is a lower bound, but the hope is that for a well trained model, this lower bound is fairly tight.\n\nReconstruction of a digit and a chicken\n\nTo test this, we train a convolutional VAE on MNIST. Note that the ELBO is the sum of a reconstruction error term and a KL divergence term. So if an image is poorly reconstructed by the VAE, it will typically have low probability. The figure below shows reconstructions from the trained VAE.\n\n\nA digit and a chicken reconstructed by a VAE trained on MNIST. As can be seen the digit is well reconstructed while the chicken is not. This suggests the chicken has low probability under the training distribution.\n\nWe can now use the VAE to predict the probability of 10000 FashionMNIST images and 10000 MNIST images under \\(p_{\\text{train}}\\). Ideally, the probabilities of FashionMNIST examples would be considerably lower than all the MNIST examples and we would get a good separation between the two. The figure below shows the results, with sorted probabilities from lowest to highest.\n\n\nFashionMNIST and MNIST examples sorted by probabilities from a VAE model.\n\nAs can be seen the separation is much cleaner than when using the maximum class probabilities \\(p(y \\vert x)\\). This shows that modeling \\(p(x)\\) can be useful for classification tasks when data different from the training data may be used at test time.\n\nConclusion\n\nIn this post we used the toy example of chickens and digits to show that a deep learning model can make confident, but meaningless, predictions on data it has never seen. Not only does a chicken get confidently classified as a 5 by an MNIST model, other natural images such as fashion items consistently fool the classifier into making high confidence predictions. We showed that modeling \\(p(x)\\) with a VAE is a simple solution that can partially mitigate this problem. However, solving this problem and, more generally, modeling uncertainty in deep learning is an important area of research.\n\nFootnotes\n1. The idea of putting a picture of a chicken through an MNIST model initially came from a question I heard on the Approximate Inference panel at NIPS 2017\n\n2. I resized MNIST from 28 by 28 to 32 by 32 for these experiments\n\n3. The word confidence is used loosely here and is not related to confidence in the statistical sense. However \\(\\max_y p(y \\vert x)\\) is commonly used to show that a model is “confident” about its predictions and this is how we use it here\n",
        "content": "When you put a picture of a chicken through a model trained on MNIST, the model is 99.9% confident that the chicken is a 5. That’s not good.\n\nThis problem does not just relate to chickens and digits but the fact that a neural net makes very confident predictions on data that does not come from the same distribution as the training data. While this example is artificial, it is common in practice for a machine learning model to be used on data that is very different from the data it was trained on. A self-driving car, for example, may encounter an unusual environment that was never seen during training. In such cases, the system should not be overly confident but instead let the driver know that it is not able to make a meaningful prediction.1\n\n\nImages from MNIST and a chicken.\n\nDiscriminative models and unseen data\n\nWhen doing classification we are often interested in building a discriminative model \\(p(y \\vert x)\\), i.e. a model of the probability of a certain label \\(y\\) (e.g. digit type) given a datapoint \\(x\\) (e.g. an image of a digit). If we use data drawn from a distribution \\(p_{\\text{train}}(x)\\) to train a discriminative model \\(p(y \\vert x)\\), how will the trained model behave when we input an \\(x\\) that is very far from \\(p_{\\text{train}}(x)\\)? For example, if we train a model to predict digit type from an image of a digit, what happens when we put a picture of a chicken through this model?\n\n\nIn the space of images, chickens lie far away from digits. This figure shows the distribution of digits in blue (corresponding to the training distribution in our case) and where an image of a chicken would lie relative to this.\n\nChicken probabilities under an MNIST model\n\nTo explore these problems, we train a simple convolutional neural network (CNN) on MNIST which gets about 98% testing accuracy. We would then like to see what happens to the output probabilities \\(p(y \\vert x)\\) of the trained model when shown images that are completely different from digits. As an example, we pass an “MNIST-ified” chicken through the model.2\n\n\nAn MNIST-ified chicken. The CNN takes in 32 by 32 grayscale images, so we transform the image of the chicken to match this.\n\nIdeally, the outputs \\(p(y \\vert x)\\) would be approximately uniform, i.e. the probability of every class would be about 10%. This would mean that the CNN has little confidence that the chicken belongs to any of the 10 classes. However, for the above picture of a chicken, the probability of the label 5 is 99.9%.\n\n\nHistograms of expected vs actual softmax class probabilities for an image of a chicken on an MNIST model.\n\nThe model is extremely confident that this chicken is the digit 5 even though, to a human, it clearly isn’t. Even worse, it is much more confident that this chicken is a 5 than many other digits that are actually a 5.\n\n\nThe model is more confident that the image on the right is a 5 than the image on the left.\n\nFashion probabilities under an MNIST model\n\nOf course, it could be that this image of a chicken is just a fluke and high confidence predictions for data outside of \\(p_{\\text{train}}(x)\\) are rare. To test this, we use the FashionMNIST dataset which contains images of various types clothing.\n\n\nMNIST and FashionMNIST examples. The images are the same size and both contain 10 classes.\n\nThese images have nothing to do with digits, so again we would hope that the model will only make low confidence predictions. We predict \\(p(y \\vert x)\\) for 10000 images from the Fashion MNIST dataset using the trained MNIST model and measure the fraction of them which have a high confidence prediction (i.e. where the maximum probability of a certain class \\(\\max_y p(y \\vert x)\\) is very high). The results are shown below:\n\n\n  63.4% of examples have more than 99% confidence\n  74.3% of examples have more than 95% confidence\n  88.9% of examples have more than 75% confidence\n\n\nAlmost two thirds of the Fashion MNIST dataset is classified as a certain digit type with more than 99% confidence. This shows that neural nets can consistently make confident predictions about unseen data and that using the output probabilities as a measure of confidence does not make much sense, at least on data that is very far from the training data.\n\nWe can also look at how confident3 predictions on the fashion items are compared to those on correctly classified digits. To do this, we draw a vertical pink line for every FashionMNIST image and a blue line for every MNIST image. We then sort the lines by the confidence of the prediction on the corresponding image. Ideally, the resulting plot would be all pink on the left and all blue on the right (i.e all MNIST examples have higher confidence than the FashionMNIST examples under an MNIST model). The actual results are shown below.\n\n\nImages sorted by confidence. The x-axis corresponds to increasing confidence and each vertical line to an image.\n\nIdeally, all FashionMNIST images would have lower confidence and so be on the left, but this is not the case.\n\nNatural adversarial examples\n\nThe chicken and fashion images can loosely be thought of as “natural” adversarial examples. Adversarial examples are typically images from a certain class (e.g. 5) that have been imperceptibly modified to be misclassified as another class (e.g. 7) with high confidence. In the same way that adversarial examples fool the machine learning model, the chicken and fashion images “fool” the model into classifying these images into a certain class with high confidence even though they do not belong to that class (or any of the classes in our case). Machine Learning systems should not only be protected from attackers that maliciously modify images but also from naturally occurring images that are far from the training distribution.\n\nModeling the data p(x)\n\nIt seems clear that we can’t solely rely on modeling \\(p(y \\vert x)\\) when data far from \\(p_{\\text{train}}(x)\\) may be used at test time. In the real world, it is often very difficult to constrain the user only to use data drawn from \\(p_{\\text{train}}(x)\\).\n\nOne way to solve this problem is to not only model \\(p(y \\vert x)\\) but to also model \\(p_{\\text{train}}(x)\\). If we can model \\(p_{\\text{train}}(x)\\) and we get a new sample \\(x_{\\text{test}}\\), we can first check whether this sample is probable under \\(p_{\\text{train}}(x)\\). If it is, we have seen something similar before so we should go ahead and predict \\(p(y \\vert x)\\), otherwise we can reject this sample.\n\n\nSimple algorithm for returning meaningful predictions.\n\nThere are several ways of modeling p(x). In this post, we will focus on variational autoencoders (VAE) which have been quite successful at modeling distributions of images.\n\nVariational Autoencoders to model p(x)\n\nVAEs are generative models that learn a joint model \\(p(x, z)\\) of the data \\(x\\) and some latent variables \\(z\\). As the name suggests, VAEs are closely related to autoencoders. VAEs work by encoding a datapoint \\(x\\) into a distribution \\(q(z \\vert x)\\) of latent variables and then sampling a latent vector \\(z\\) from this distribution. The sample \\(z\\) is then decoded into a reconstruction of the encoded data \\(x\\). The encoder and decoder are typically neural networks.\n\n\nSketch of VAE architecture, sampling is shown with dashed lines.\n\nInterestingly, VAEs optimize a lower bound on \\(\\log p(x)\\) called the Evidence Lower Bound (ELBO).\n\n[\\log p(x) &gt;= \\text{ELBO} = - \\text{VAE loss}]\n\nSo after training a VAE on data from \\(p_{\\text{train}}\\), we can calculate the loss on a new example \\(x_{\\text{test}}\\) and obtain a lower bound on the log likelihood of that example under \\(p_{\\text{train}}\\). Of course, this is a lower bound, but the hope is that for a well trained model, this lower bound is fairly tight.\n\nReconstruction of a digit and a chicken\n\nTo test this, we train a convolutional VAE on MNIST. Note that the ELBO is the sum of a reconstruction error term and a KL divergence term. So if an image is poorly reconstructed by the VAE, it will typically have low probability. The figure below shows reconstructions from the trained VAE.\n\n\nA digit and a chicken reconstructed by a VAE trained on MNIST. As can be seen the digit is well reconstructed while the chicken is not. This suggests the chicken has low probability under the training distribution.\n\nWe can now use the VAE to predict the probability of 10000 FashionMNIST images and 10000 MNIST images under \\(p_{\\text{train}}\\). Ideally, the probabilities of FashionMNIST examples would be considerably lower than all the MNIST examples and we would get a good separation between the two. The figure below shows the results, with sorted probabilities from lowest to highest.\n\n\nFashionMNIST and MNIST examples sorted by probabilities from a VAE model.\n\nAs can be seen the separation is much cleaner than when using the maximum class probabilities \\(p(y \\vert x)\\). This shows that modeling \\(p(x)\\) can be useful for classification tasks when data different from the training data may be used at test time.\n\nConclusion\n\nIn this post we used the toy example of chickens and digits to show that a deep learning model can make confident, but meaningless, predictions on data it has never seen. Not only does a chicken get confidently classified as a 5 by an MNIST model, other natural images such as fashion items consistently fool the classifier into making high confidence predictions. We showed that modeling \\(p(x)\\) with a VAE is a simple solution that can partially mitigate this problem. However, solving this problem and, more generally, modeling uncertainty in deep learning is an important area of research.\n\nFootnotes\n1. The idea of putting a picture of a chicken through an MNIST model initially came from a question I heard on the Approximate Inference panel at NIPS 2017\n\n2. I resized MNIST from 28 by 28 to 32 by 32 for these experiments\n\n3. The word confidence is used loosely here and is not related to confidence in the statistical sense. However \\(\\max_y p(y \\vert x)\\) is commonly used to show that a model is “confident” about its predictions and this is how we use it here\n",
        "url": "/2018/03/14/mnist-chicken/"
      },
    
  
  
  
  {
    "title": "Hello!",
    "excerpt": "\n",
    "content": "\n  \n\n  \n    Chen Jin\n  \n  \n    \n      twitter\n\n    \n    &nbsp;\n    \n      github\n\n    \n    &nbsp;\n    \n      linkedin\n\n    \n    &nbsp;\n    \n      scholar\n\n    \n  \n\n\nHi! I’m a postdoctoral research fellow in the Centre for Medical Image Computing (CMIC) at University College London, working with Dr Thomy Mertzanidou, Prof. Daniel Alexander and Dr Marnix Jansen.\n\nMy recent research interests include efficient deep learning based analysis of large images like ultra-high resolution 2D images, 3D image or video streams in healthcare applications. I’m also interested in the multiple scale and multi modality learning, particularly in medical or geographical image domain.\n\nPreviously, I obtained my PhD in the Institute of GeoEnergy Engineering, Heriot-Watt University. I’ve worked on Digital Rock Analysis, involving machine learning based multi-scale 2D/3D image analysis, segmentation, registration, fusion, reconstruction and fluid flow simulation.\n\nDuring PhD, I also worked as a research intern at Canadian Natural Resources International (UK) in Aberdeen. I was responsible for building the physical reservoir scale models and investigated fluid flow behaviours through simulation, and with the fortune to present the team and helped decisions worth millions.\n\nContact\n\nMy email is chen.jin (at) ucl.ac.uk\n",
    "url": "/"
  },
  
  {
    "title": "Publications",
    "excerpt": "\n",
    "content": "Constructing …\n\n",
    "url": "/publications/"
  },
  
  {
    "title": "Resume",
    "excerpt": "\n",
    "content": "For more details, you can download the pdf version of my resume pdf\nhere (updated Nov 2020).\n\nEducation 🎓\n\n\n\n\n  \n    \n  \n\n  \n    \n      Heriot-Watt University\n      2018 - 2021\n    \n    PhD Computational Geoscience\n    \n      \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n  \n    \n      Heriot-Watt University\n      2014 - 2016\n    \n    MSc Petroleum Engineering\n    \n      \n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n  \n    \n      China University of Petroleum\n      2010 - 2014\n    \n    BSc Oil and Gas Engineering\n    \n      \n    \n  \n\n\n\nExperience 👨‍💼\n\n\n\n\n  \n    \n  \n\n  \n    \n      University College London\n      Mar 2019 - Present\n    \n    Research Associate\n    \n      Develop deep learning model for histopathology image analysis and multiple scale-modality mapping for Histology-MRI project\n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n  \n    \n      Heriot-Watt University\n      Nov 2017 - Nov 2018\n    \n    Research Associate\n    \n      3D images modelling and reconstruction for mineral at nano-scale\n    \n  \n\n\n\n\n\n\n  \n    \n  \n\n  \n    \n      Canadian Natural Resources International (UK) Limited\n      June 2016 - July 2018\n    \n    Intern Student\n    \n      Performed and presented numerical simulation analysis to development team helped on decisions worth millions\n    \n  \n\n\n\n",
    "url": "/resume/"
  }
  
]

