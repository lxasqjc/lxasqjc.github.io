<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en-GB"><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" hreflang="en-GB" /><updated>2020-12-01T12:47:12+00:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Chen Jin</title><subtitle>Machine Learning</subtitle><author><name>ChenJin</name></author><entry><title type="html">Passing a Chicken through an MNIST Model</title><link href="http://localhost:4000/2018/03/14/mnist-chicken/" rel="alternate" type="text/html" title="Passing a Chicken through an MNIST Model" /><published>2018-03-14T00:00:00+00:00</published><updated>2018-03-14T00:00:00+00:00</updated><id>http://localhost:4000/2018/03/14/mnist-chicken</id><content type="html" xml:base="http://localhost:4000/2018/03/14/mnist-chicken/">&lt;p&gt;When you put a picture of a chicken through a model trained on &lt;a href=&quot;https://en.wikipedia.org/wiki/MNIST_database&quot;&gt;MNIST&lt;/a&gt;, the model is 99.9% confident that the chicken is a 5. That’s not good.&lt;/p&gt;

&lt;p&gt;This problem does not just relate to chickens and digits but the fact that a neural net makes very confident predictions on data that does not come from the same distribution as the training data. While this example is artificial, it is common in practice for a machine learning model to be used on data that is very different from the data it was trained on. A self-driving car, for example, may encounter an unusual environment that was never seen during training. In such cases, the system should not be overly confident but instead let the driver know that it is not able to make a meaningful prediction.&lt;sup&gt;&lt;a href=&quot;#myfootnote1&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/mnist-grid-with-chicken.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:80%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;Images from MNIST and a chicken.&lt;/p&gt;

&lt;h2 id=&quot;discriminative-models-and-unseen-data&quot;&gt;Discriminative models and unseen data&lt;/h2&gt;

&lt;p&gt;When doing classification we are often interested in building a discriminative model \(p(y \vert x)\), i.e. a model of the probability of a certain label \(y\) (e.g. digit type) given a datapoint \(x\) (e.g. an image of a digit). If we use data drawn from a distribution \(p_{\text{train}}(x)\) to train a discriminative model \(p(y \vert x)\), how will the trained model behave when we input an \(x\) that is very far from \(p_{\text{train}}(x)\)? For example, if we train a model to predict digit type from an image of a digit, what happens when we put a picture of a chicken through this model?&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/digits-chicken-prob-map.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:100%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;In the space of images, chickens lie far away from digits. This figure shows the distribution of digits in blue (corresponding to the training distribution in our case) and where an image of a chicken would lie relative to this.&lt;/p&gt;

&lt;h2 id=&quot;chicken-probabilities-under-an-mnist-model&quot;&gt;Chicken probabilities under an MNIST model&lt;/h2&gt;

&lt;p&gt;To explore these problems, we train a simple convolutional neural network (CNN) on MNIST which gets about 98% testing accuracy. We would then like to see what happens to the output probabilities \(p(y \vert x)\) of the trained model when shown images that are completely different from digits. As an example, we pass an “MNIST-ified” chicken through the model.&lt;sup&gt;&lt;a href=&quot;#myfootnote2&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/mnistify-chicken.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:100%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;An MNIST-ified chicken. The CNN takes in 32 by 32 grayscale images, so we transform the image of the chicken to match this.&lt;/p&gt;

&lt;p&gt;Ideally, the outputs \(p(y \vert x)\) would be approximately uniform, i.e. the probability of every class would be about 10%. This would mean that the CNN has little confidence that the chicken belongs to any of the 10 classes. However, for the above picture of a chicken, the probability of the label 5 is &lt;strong&gt;99.9%&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/expected-vs-actual-softmax.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:60%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;Histograms of expected vs actual softmax class probabilities for an image of a chicken on an MNIST model.&lt;/p&gt;

&lt;p&gt;The model is extremely confident that this chicken is the digit 5 even though, to a human, it clearly isn’t. Even worse, it is much more confident that this chicken is a 5 than many other digits that are actually a 5.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/five-and-chicken-conf.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:50%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;The model is more confident that the image on the right is a 5 than the image on the left.&lt;/p&gt;

&lt;h2 id=&quot;fashion-probabilities-under-an-mnist-model&quot;&gt;Fashion probabilities under an MNIST model&lt;/h2&gt;

&lt;p&gt;Of course, it could be that this image of a chicken is just a fluke and high confidence predictions for data outside of \(p_{\text{train}}(x)\) are rare. To test this, we use the &lt;a href=&quot;https://github.com/zalandoresearch/fashion-mnist&quot;&gt;FashionMNIST&lt;/a&gt; dataset which contains images of various types clothing.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/mnist-and-fashion-examples.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:60%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;MNIST and FashionMNIST examples. The images are the same size and both contain 10 classes.&lt;/p&gt;

&lt;p&gt;These images have nothing to do with digits, so again we would hope that the model will only make low confidence predictions. We predict \(p(y \vert x)\) for 10000 images from the Fashion MNIST dataset using the trained MNIST model and measure the fraction of them which have a high confidence prediction (i.e. where the maximum probability of a certain class \(\max_y p(y \vert x)\) is very high). The results are shown below:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;63.4%&lt;/strong&gt; of examples have more than &lt;strong&gt;99%&lt;/strong&gt; confidence&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;74.3%&lt;/strong&gt; of examples have more than &lt;strong&gt;95%&lt;/strong&gt; confidence&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;88.9%&lt;/strong&gt; of examples have more than &lt;strong&gt;75%&lt;/strong&gt; confidence&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Almost two thirds of the Fashion MNIST dataset is classified as a certain digit type with more than 99% confidence. This shows that neural nets can consistently make confident predictions about unseen data and that using the output probabilities as a measure of confidence does not make much sense, at least on data that is very far from the training data.&lt;/p&gt;

&lt;p&gt;We can also look at how confident&lt;sup&gt;&lt;a href=&quot;#myfootnote3&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; predictions on the fashion items are compared to those on correctly classified digits. To do this, we draw a vertical pink line for every FashionMNIST image and a blue line for every MNIST image. We then sort the lines by the confidence of the prediction on the corresponding image. Ideally, the resulting plot would be all pink on the left and all blue on the right (i.e all MNIST examples have higher confidence than the FashionMNIST examples under an MNIST model). The actual results are shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/cnn-confidence.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:100%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;Images sorted by confidence. The x-axis corresponds to increasing confidence and each vertical line to an image.&lt;/p&gt;

&lt;p&gt;Ideally, all FashionMNIST images would have lower confidence and so be on the left, but this is not the case.&lt;/p&gt;

&lt;h2 id=&quot;natural-adversarial-examples&quot;&gt;Natural adversarial examples&lt;/h2&gt;

&lt;p&gt;The chicken and fashion images can loosely be thought of as “natural” adversarial examples. &lt;a href=&quot;https://arxiv.org/pdf/1312.6199.pdf&quot;&gt;Adversarial examples&lt;/a&gt; are typically images from a certain class (e.g. 5) that have been imperceptibly modified to be misclassified as another class (e.g. 7) with high confidence. In the same way that adversarial examples fool the machine learning model, the chicken and fashion images “fool” the model into classifying these images into a certain class with high confidence even though they do not belong to that class (or any of the classes in our case). Machine Learning systems should not only be protected from attackers that maliciously modify images but also from naturally occurring images that are far from the training distribution.&lt;/p&gt;

&lt;h2 id=&quot;modeling-the-data-px&quot;&gt;Modeling the data p(x)&lt;/h2&gt;

&lt;p&gt;It seems clear that we can’t solely rely on modeling \(p(y \vert x)\) when data far from \(p_{\text{train}}(x)\) may be used at test time. In the real world, it is often very difficult to constrain the user only to use data drawn from \(p_{\text{train}}(x)\).&lt;/p&gt;

&lt;p&gt;One way to solve this problem is to not only model \(p(y \vert x)\) but to also model \(p_{\text{train}}(x)\). If we can model \(p_{\text{train}}(x)\) and we get a new sample \(x_{\text{test}}\), we can first check whether this sample is probable under \(p_{\text{train}}(x)\). If it is, we have seen something similar before so we should go ahead and predict \(p(y \vert x)\), otherwise we can reject this sample.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/algorithm.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:40%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;Simple algorithm for returning meaningful predictions.&lt;/p&gt;

&lt;p&gt;There are several ways of modeling p(x). In this post, we will focus on &lt;a href=&quot;https://arxiv.org/abs/1312.6114&quot;&gt;variational autoencoders&lt;/a&gt; (VAE) which have been quite successful at modeling distributions of images.&lt;/p&gt;

&lt;h2 id=&quot;variational-autoencoders-to-model-px&quot;&gt;Variational Autoencoders to model p(x)&lt;/h2&gt;

&lt;p&gt;VAEs are &lt;a href=&quot;https://en.wikipedia.org/wiki/Generative_model&quot;&gt;generative models&lt;/a&gt; that learn a joint model \(p(x, z)\) of the data \(x\) and some &lt;a href=&quot;https://en.wikipedia.org/wiki/Latent_variable&quot;&gt;latent variables&lt;/a&gt; \(z\). As the name suggests, VAEs are closely related to &lt;a href=&quot;https://en.wikipedia.org/wiki/Autoencoder&quot;&gt;autoencoders&lt;/a&gt;. VAEs work by encoding a datapoint \(x\) into a distribution \(q(z \vert x)\) of latent variables and then sampling a latent vector \(z\) from this distribution. The sample \(z\) is then decoded into a reconstruction of the encoded data \(x\). The encoder and decoder are typically neural networks.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/vae.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:40%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;Sketch of VAE architecture, sampling is shown with dashed lines.&lt;/p&gt;

&lt;p&gt;Interestingly, VAEs optimize a lower bound on \(\log p(x)\) called the &lt;a href=&quot;https://arxiv.org/pdf/1601.00670.pdf&quot;&gt;Evidence Lower Bound&lt;/a&gt; (ELBO).&lt;/p&gt;

\[\log p(x) &amp;gt;= \text{ELBO} = - \text{VAE loss}\]

&lt;p&gt;So after training a VAE on data from \(p_{\text{train}}\), we can calculate the loss on a new example \(x_{\text{test}}\) and obtain a lower bound on the log likelihood of that example under \(p_{\text{train}}\). Of course, this is a lower bound, but the hope is that for a well trained model, this lower bound is fairly tight.&lt;/p&gt;

&lt;h2 id=&quot;reconstruction-of-a-digit-and-a-chicken&quot;&gt;Reconstruction of a digit and a chicken&lt;/h2&gt;

&lt;p&gt;To test this, we train a convolutional VAE on MNIST. Note that the ELBO is the sum of a &lt;a href=&quot;https://arxiv.org/abs/1606.05908&quot;&gt;reconstruction error term and a KL divergence&lt;/a&gt; term. So if an image is poorly reconstructed by the VAE, it will typically have low probability. The figure below shows reconstructions from the trained VAE.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/reconstructed-chicken.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:50%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;A digit and a chicken reconstructed by a VAE trained on MNIST. As can be seen the digit is well reconstructed while the chicken is not. This suggests the chicken has low probability under the training distribution.&lt;/p&gt;

&lt;p&gt;We can now use the VAE to predict the probability of 10000 FashionMNIST images and 10000 MNIST images under \(p_{\text{train}}\). Ideally, the probabilities of FashionMNIST examples would be considerably lower than all the MNIST examples and we would get a good separation between the two. The figure below shows the results, with sorted probabilities from lowest to highest.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/mnist-chicken/vae-confidence.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:100%;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;FashionMNIST and MNIST examples sorted by probabilities from a VAE model.&lt;/p&gt;

&lt;p&gt;As can be seen the separation is much cleaner than when using the maximum class probabilities \(p(y \vert x)\). This shows that modeling \(p(x)\) can be useful for classification tasks when data different from the training data may be used at test time.&lt;/p&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;In this post we used the toy example of chickens and digits to show that a deep learning model can make confident, but meaningless, predictions on data it has never seen. Not only does a chicken get confidently classified as a 5 by an MNIST model, other natural images such as fashion items consistently fool the classifier into making high confidence predictions. We showed that modeling \(p(x)\) with a VAE is a simple solution that can partially mitigate this problem. However, solving this problem and, more generally, modeling &lt;a href=&quot;http://mlg.eng.cam.ac.uk/yarin/thesis/thesis.pdf&quot;&gt;uncertainty in deep learning&lt;/a&gt; is an important area of research.&lt;/p&gt;

&lt;h4 id=&quot;footnotes&quot;&gt;Footnotes&lt;/h4&gt;
&lt;p&gt;&lt;a name=&quot;footnote1&quot;&gt;1&lt;/a&gt;. The idea of putting a picture of a chicken through an MNIST model initially came from a question I heard on the &lt;a href=&quot;http://approximateinference.org/&quot;&gt;Approximate Inference&lt;/a&gt; panel at NIPS 2017&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;footnote2&quot;&gt;2&lt;/a&gt;. I resized MNIST from 28 by 28 to 32 by 32 for these experiments&lt;/p&gt;

&lt;p&gt;&lt;a name=&quot;footnote3&quot;&gt;3&lt;/a&gt;. The word confidence is used loosely here and is not related to confidence in the &lt;a href=&quot;https://en.wikipedia.org/wiki/Confidence_interval&quot;&gt;statistical sense&lt;/a&gt;. However \(\max_y p(y \vert x)\) is commonly used to show that a model is “confident” about its predictions and this is how we use it here&lt;/p&gt;</content><author><name>ChenJin</name></author><summary type="html">When you put a picture of a chicken through a model trained on MNIST, the model is 99.9% confident that the chicken is a 5. That’s not good. This problem does not just relate to chickens and digits but the fact that a neural net makes very confident predictions on data that does not come from the same distribution as the training data. While this example is artificial, it is common in practice for a machine learning model to be used on data that is very different from the data it was trained on. A self-driving car, for example, may encounter an unusual environment that was never seen during training. In such cases, the system should not be overly confident but instead let the driver know that it is not able to make a meaningful prediction.1 Images from MNIST and a chicken. Discriminative models and unseen data When doing classification we are often interested in building a discriminative model \(p(y \vert x)\), i.e. a model of the probability of a certain label \(y\) (e.g. digit type) given a datapoint \(x\) (e.g. an image of a digit). If we use data drawn from a distribution \(p_{\text{train}}(x)\) to train a discriminative model \(p(y \vert x)\), how will the trained model behave when we input an \(x\) that is very far from \(p_{\text{train}}(x)\)? For example, if we train a model to predict digit type from an image of a digit, what happens when we put a picture of a chicken through this model? In the space of images, chickens lie far away from digits. This figure shows the distribution of digits in blue (corresponding to the training distribution in our case) and where an image of a chicken would lie relative to this. Chicken probabilities under an MNIST model To explore these problems, we train a simple convolutional neural network (CNN) on MNIST which gets about 98% testing accuracy. We would then like to see what happens to the output probabilities \(p(y \vert x)\) of the trained model when shown images that are completely different from digits. As an example, we pass an “MNIST-ified” chicken through the model.2 An MNIST-ified chicken. The CNN takes in 32 by 32 grayscale images, so we transform the image of the chicken to match this. Ideally, the outputs \(p(y \vert x)\) would be approximately uniform, i.e. the probability of every class would be about 10%. This would mean that the CNN has little confidence that the chicken belongs to any of the 10 classes. However, for the above picture of a chicken, the probability of the label 5 is 99.9%. Histograms of expected vs actual softmax class probabilities for an image of a chicken on an MNIST model. The model is extremely confident that this chicken is the digit 5 even though, to a human, it clearly isn’t. Even worse, it is much more confident that this chicken is a 5 than many other digits that are actually a 5. The model is more confident that the image on the right is a 5 than the image on the left. Fashion probabilities under an MNIST model Of course, it could be that this image of a chicken is just a fluke and high confidence predictions for data outside of \(p_{\text{train}}(x)\) are rare. To test this, we use the FashionMNIST dataset which contains images of various types clothing. MNIST and FashionMNIST examples. The images are the same size and both contain 10 classes. These images have nothing to do with digits, so again we would hope that the model will only make low confidence predictions. We predict \(p(y \vert x)\) for 10000 images from the Fashion MNIST dataset using the trained MNIST model and measure the fraction of them which have a high confidence prediction (i.e. where the maximum probability of a certain class \(\max_y p(y \vert x)\) is very high). The results are shown below: 63.4% of examples have more than 99% confidence 74.3% of examples have more than 95% confidence 88.9% of examples have more than 75% confidence Almost two thirds of the Fashion MNIST dataset is classified as a certain digit type with more than 99% confidence. This shows that neural nets can consistently make confident predictions about unseen data and that using the output probabilities as a measure of confidence does not make much sense, at least on data that is very far from the training data. We can also look at how confident3 predictions on the fashion items are compared to those on correctly classified digits. To do this, we draw a vertical pink line for every FashionMNIST image and a blue line for every MNIST image. We then sort the lines by the confidence of the prediction on the corresponding image. Ideally, the resulting plot would be all pink on the left and all blue on the right (i.e all MNIST examples have higher confidence than the FashionMNIST examples under an MNIST model). The actual results are shown below. Images sorted by confidence. The x-axis corresponds to increasing confidence and each vertical line to an image. Ideally, all FashionMNIST images would have lower confidence and so be on the left, but this is not the case. Natural adversarial examples The chicken and fashion images can loosely be thought of as “natural” adversarial examples. Adversarial examples are typically images from a certain class (e.g. 5) that have been imperceptibly modified to be misclassified as another class (e.g. 7) with high confidence. In the same way that adversarial examples fool the machine learning model, the chicken and fashion images “fool” the model into classifying these images into a certain class with high confidence even though they do not belong to that class (or any of the classes in our case). Machine Learning systems should not only be protected from attackers that maliciously modify images but also from naturally occurring images that are far from the training distribution. Modeling the data p(x) It seems clear that we can’t solely rely on modeling \(p(y \vert x)\) when data far from \(p_{\text{train}}(x)\) may be used at test time. In the real world, it is often very difficult to constrain the user only to use data drawn from \(p_{\text{train}}(x)\). One way to solve this problem is to not only model \(p(y \vert x)\) but to also model \(p_{\text{train}}(x)\). If we can model \(p_{\text{train}}(x)\) and we get a new sample \(x_{\text{test}}\), we can first check whether this sample is probable under \(p_{\text{train}}(x)\). If it is, we have seen something similar before so we should go ahead and predict \(p(y \vert x)\), otherwise we can reject this sample. Simple algorithm for returning meaningful predictions. There are several ways of modeling p(x). In this post, we will focus on variational autoencoders (VAE) which have been quite successful at modeling distributions of images. Variational Autoencoders to model p(x) VAEs are generative models that learn a joint model \(p(x, z)\) of the data \(x\) and some latent variables \(z\). As the name suggests, VAEs are closely related to autoencoders. VAEs work by encoding a datapoint \(x\) into a distribution \(q(z \vert x)\) of latent variables and then sampling a latent vector \(z\) from this distribution. The sample \(z\) is then decoded into a reconstruction of the encoded data \(x\). The encoder and decoder are typically neural networks. Sketch of VAE architecture, sampling is shown with dashed lines. Interestingly, VAEs optimize a lower bound on \(\log p(x)\) called the Evidence Lower Bound (ELBO). \[\log p(x) &amp;gt;= \text{ELBO} = - \text{VAE loss}\] So after training a VAE on data from \(p_{\text{train}}\), we can calculate the loss on a new example \(x_{\text{test}}\) and obtain a lower bound on the log likelihood of that example under \(p_{\text{train}}\). Of course, this is a lower bound, but the hope is that for a well trained model, this lower bound is fairly tight. Reconstruction of a digit and a chicken To test this, we train a convolutional VAE on MNIST. Note that the ELBO is the sum of a reconstruction error term and a KL divergence term. So if an image is poorly reconstructed by the VAE, it will typically have low probability. The figure below shows reconstructions from the trained VAE. A digit and a chicken reconstructed by a VAE trained on MNIST. As can be seen the digit is well reconstructed while the chicken is not. This suggests the chicken has low probability under the training distribution. We can now use the VAE to predict the probability of 10000 FashionMNIST images and 10000 MNIST images under \(p_{\text{train}}\). Ideally, the probabilities of FashionMNIST examples would be considerably lower than all the MNIST examples and we would get a good separation between the two. The figure below shows the results, with sorted probabilities from lowest to highest. FashionMNIST and MNIST examples sorted by probabilities from a VAE model. As can be seen the separation is much cleaner than when using the maximum class probabilities \(p(y \vert x)\). This shows that modeling \(p(x)\) can be useful for classification tasks when data different from the training data may be used at test time. Conclusion In this post we used the toy example of chickens and digits to show that a deep learning model can make confident, but meaningless, predictions on data it has never seen. Not only does a chicken get confidently classified as a 5 by an MNIST model, other natural images such as fashion items consistently fool the classifier into making high confidence predictions. We showed that modeling \(p(x)\) with a VAE is a simple solution that can partially mitigate this problem. However, solving this problem and, more generally, modeling uncertainty in deep learning is an important area of research. Footnotes 1. The idea of putting a picture of a chicken through an MNIST model initially came from a question I heard on the Approximate Inference panel at NIPS 2017 2. I resized MNIST from 28 by 28 to 32 by 32 for these experiments 3. The word confidence is used loosely here and is not related to confidence in the statistical sense. However \(\max_y p(y \vert x)\) is commonly used to show that a model is “confident” about its predictions and this is how we use it here</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/default-social-image.png" /><media:content medium="image" url="http://localhost:4000/assets/default-social-image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry><entry><title type="html">Interactive Visualization of Optimization Algorithms in Deep Learning</title><link href="http://localhost:4000/2018/01/24/optimization-visualization/" rel="alternate" type="text/html" title="Interactive Visualization of Optimization Algorithms in Deep Learning" /><published>2018-01-24T00:00:00+00:00</published><updated>2018-01-24T00:00:00+00:00</updated><id>http://localhost:4000/2018/01/24/optimization-visualization</id><content type="html" xml:base="http://localhost:4000/2018/01/24/optimization-visualization/">&lt;p&gt;Optimization on non convex functions in high dimensional spaces, like those encountered in deep learning, can be hard to visualize. However, we can learn a lot from visualizing optimization paths on simple 2d non convex functions.&lt;/p&gt;

&lt;p style=&quot;text-align: center; font-weight: bold;&quot;&gt;Click anywhere on the function contour to start a minimization.&lt;/p&gt;

&lt;div id=&quot;optim-viz&quot;&gt;
&lt;/div&gt;

&lt;p&gt;You can toggle the different algorithms by clicking the circles in the lower bar. The code is available &lt;a href=&quot;https://bl.ocks.org/EmilienDupont/aaf429be5705b219aaaf8d691e27ca87&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;observations&quot;&gt;Observations&lt;/h2&gt;

&lt;p&gt;The above function is given by&lt;/p&gt;

\[f(x, y) =  x^2 + y^2 - a e^{-\frac{(x - 1)^2 + y^2}{c}} - b e^{-\frac{(x + 1)^2 + y^2}{d}}\]

&lt;p&gt;It is basically a quadratic “bowl” with two gaussians creating minima at (1, 0) and (-1, 0) respectively. The size of these minima is controlled by the \(a\) and \(b\) parameters.&lt;/p&gt;

&lt;h3 id=&quot;different-minima&quot;&gt;Different minima&lt;/h3&gt;

&lt;p&gt;Starting from the same point, different algorithms will converge to different minima. Often, SGD and SGD with momentum will converge to the poorer minimum (the one on the right) while RMSProp and Adam will converge to the global minimum. For this particular function, Adam is the algorithm that converges to the global minimum from the most initializations.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/optim_viz_only_adam.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:500px;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;Only Adam (in green) converges to the global minimum.&lt;/p&gt;

&lt;h3 id=&quot;the-effects-of-momentum&quot;&gt;The effects of momentum&lt;/h3&gt;

&lt;p&gt;Augmenting SGD with momentum has &lt;a href=&quot;https://distill.pub/2017/momentum/&quot;&gt;many advantages&lt;/a&gt; and often works better than the other standard algorithms for an appropriately chosen learning rate (check out this &lt;a href=&quot;https://arxiv.org/abs/1705.08292&quot;&gt;paper&lt;/a&gt; for more details). However, with the wrong learning rate, SGD with momentum can overshoot minima and this often leads to a spiraling pattern around the minimum.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/optim_viz_momentum.png&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:500px;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;SGD with momentum spiraling towards the minimum.&lt;/p&gt;

&lt;h3 id=&quot;standard-sgd-does-not-get-you-far&quot;&gt;Standard SGD does not get you far&lt;/h3&gt;

&lt;p&gt;SGD without momentum consistently performs the worst. The learning rate for SGD on the visualization is set to be artificially high (an order of magnitude higher than the other algorithms) in order for the optimization to converge in a reasonable amount of time.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;classic-optimization-test-functions&quot;&gt;Classic optimization test functions&lt;/h2&gt;

&lt;p&gt;There are many famous &lt;a href=&quot;https://en.wikipedia.org/wiki/Test_functions_for_optimization&quot;&gt;test functions&lt;/a&gt; for optimization which are useful for testing convergence, precision, robustness and performance of optimization algorithms. They also exhibit interesting behaviour which does not appear in the above function.&lt;/p&gt;

&lt;h3 id=&quot;rastrigin&quot;&gt;Rastrigin&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The visualization for this function can be found &lt;a href=&quot;https://observablehq.com/@emiliendupont/optimization-on-rastrigin-function&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A &lt;a href=&quot;https://en.wikipedia.org/wiki/Rastrigin_function&quot;&gt;Rastrigin function&lt;/a&gt; is a quadratic bowl overlayed with a grid of sine bumps creating a large number of local minima.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/optim_viz_rastrigin.gif&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:640px;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;SGD with momentum reaches the global optimum while all other algorithms get stuck in the same local minimum.&lt;/p&gt;

&lt;p&gt;In this example, SGD with momentum outperforms all other algorithms using the default parameter settings. The speed built up from the momentum allows it to power through the sine bumps and converge to the global minimum when other algorithms don’t. Of course, this would not necessarily be the case if the sine bumps had been scaled or spaced differently. Indeed, on the first function in this post, Adam performed the best while SGD with momentum performs the best on the Rastrigin function. This shows that there is no single algorithm that will perform the best on all functions, even in simple 2D cases.&lt;/p&gt;

&lt;h3 id=&quot;rosenbrock&quot;&gt;Rosenbrock&lt;/h3&gt;

&lt;blockquote&gt;
  &lt;p&gt;The visualization for this function can be found &lt;a href=&quot;https://observablehq.com/@emiliendupont/optimization-on-rosenbrock-function&quot;&gt;here&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The &lt;a href=&quot;https://en.wikipedia.org/wiki/Rosenbrock_function&quot;&gt;Rosenbrock function&lt;/a&gt; has a single global minimum inside a parabolic shaped valley. Most algorithms rapidly converge to this valley, but it is typically difficult to converge to the global minimum within this valley.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;http://localhost:4000/imgs/optim_viz_rosenbrock.gif&quot; style=&quot;display:block; margin-left:auto; margin-right:auto; width:640px;&quot; /&gt;&lt;/p&gt;
&lt;p style=&quot;text-align: center; font-style: italic; font-size: 80%;&quot;&gt;All algorithms find the global minimum but through very different paths&lt;/p&gt;

&lt;p&gt;While all algorithms converge to the optimum, the adaptive and non adaptive optimization algorithms approach the minimum through different paths. In higher dimensional problems, like in deep learning, different optimization algorithms will likely explore very different areas of parameter space.&lt;/p&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;

&lt;p&gt;Optimization algorithms can exhibit interesting behaviour, even on simple 2d functions. Of course, there are also many phenomena which we cannot hope to visualize on simple 2d problems. &lt;a href=&quot;http://opt-ml.org/&quot;&gt;Understanding&lt;/a&gt; and &lt;a href=&quot;https://arxiv.org/abs/1712.09913&quot;&gt;visualizing&lt;/a&gt; optimization in deep learning in general is an active area of research. New optimization algorithms, like &lt;a href=&quot;https://arxiv.org/abs/1611.01505&quot;&gt;Eve&lt;/a&gt; or &lt;a href=&quot;https://arxiv.org/abs/1706.03471&quot;&gt;YellowFin&lt;/a&gt;, are also being developed. It would be interesting to modify the above code to visualize these more recent algorithms, although it is unclear whether they would differ significantly from momentum SGD on these toy problems.&lt;/p&gt;

&lt;style&gt;
.sgd {
    stroke: black;
}

.momentum {
    stroke: blue;
}

.rmsprop {
    stroke: red;
}

.adam {
    stroke: green;
}

.SGD {
    fill: black;
}

.Momentum {
    fill: blue;
}

.RMSProp {
    fill: red;
}

.Adam {
    fill: green;
}

circle:hover {
  fill-opacity: .3;
}
&lt;/style&gt;

&lt;script src=&quot;https://d3js.org/d3.v4.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://d3js.org/d3-contour.v1.min.js&quot;&gt;&lt;/script&gt;

&lt;script src=&quot;https://d3js.org/d3-scale-chromatic.v1.min.js&quot;&gt;&lt;/script&gt;

&lt;script&gt;

var width = 720,
    height = 500,
    nx = parseInt(width / 5), // grid sizes
    ny = parseInt(height / 5),
    h = 1e-7, // step used when approximating gradients
    drawing_time = 30; // max time to run optimization

var svg = d3.select(&quot;#optim-viz&quot;)
            .append(&quot;svg&quot;)
            .attr(&quot;width&quot;, width)
            .attr(&quot;height&quot;, height);

// Parameters describing where function is defined
var domain_x = [-2, 2],
    domain_y = [-2, 2],
    domain_f = [-2, 8],
    contour_step = 0.5; // Step size of contour plot

var scale_x = d3.scaleLinear()
                .domain([0, width])
                .range(domain_x);

var scale_y = d3.scaleLinear()
                .domain([0, height])
                .range(domain_y);

var thresholds = d3.range(domain_f[0], domain_f[1], contour_step);

var color_scale = d3.scaleLinear()
    .domain(d3.extent(thresholds))
    .interpolate(function() { return d3.interpolateYlGnBu; });

var function_g = svg.append(&quot;g&quot;).on(&quot;mousedown&quot;, mousedown),
    gradient_path_g = svg.append(&quot;g&quot;),
    menu_g = svg.append(&quot;g&quot;);

// Set up the function and gradients

// Value of f at (x, y)
function f(x, y) {
    return -2 * Math.exp(-((x - 1) * (x - 1) + y * y) / .2) + -3 * Math.exp(-((x + 1) * (x + 1) + y * y) / .2) + x * x + y * y;
}

// Returns gradient of f at (x, y)
function grad_f(x,y) {
    var grad_x = (f(x + h, y) - f(x, y)) / h
        grad_y = (f(x, y + h) - f(x, y)) / h
    return [grad_x, grad_y];
}


// Returns values of f(x,y) at each point on grid as 1 dim array.
function get_f_values(nx, ny) {
    var grid = new Array(nx * ny);
    for (i = 0; i &lt; nx; i++) {
        for (j = 0; j &lt; ny; j++) {
            var x = scale_x( parseFloat(i) / nx * width ),
                y = scale_y( parseFloat(j) / ny * height );
            // Set value at ordering expected by d3.contour
            grid[i + j * nx] = f(x, y);
        }
    }
    return grid;
}

// Set up the contour plot

var contours = d3.contours()
    .size([nx, ny])
    .thresholds(thresholds);

var f_values = get_f_values(nx, ny);

function_g.selectAll(&quot;path&quot;)
          .data(contours(f_values))
          .enter().append(&quot;path&quot;)
          .attr(&quot;d&quot;, d3.geoPath(d3.geoIdentity().scale(width / nx)))
          .attr(&quot;fill&quot;, function(d) { return color_scale(d.value); })
          .attr(&quot;stroke&quot;, &quot;none&quot;);

// Set up buttons

var draw_bool = {&quot;SGD&quot; : true, &quot;Momentum&quot; : true, &quot;RMSProp&quot; : true, &quot;Adam&quot; : true};

var buttons = [&quot;SGD&quot;, &quot;Momentum&quot;, &quot;RMSProp&quot;, &quot;Adam&quot;];

menu_g.append(&quot;rect&quot;)
      .attr(&quot;x&quot;, 0)
      .attr(&quot;y&quot;, height - 40)
      .attr(&quot;width&quot;, width)
      .attr(&quot;height&quot;, 40)
      .attr(&quot;fill&quot;, &quot;white&quot;)
      .attr(&quot;opacity&quot;, 0.2);

menu_g.selectAll(&quot;circle&quot;)
      .data(buttons)
      .enter()
      .append(&quot;circle&quot;)
      .attr(&quot;cx&quot;, function(d,i) { return width/4 * (i + 0.25);} )
      .attr(&quot;cy&quot;, height - 20)
      .attr(&quot;r&quot;, 10)
      .attr(&quot;stroke-width&quot;, 0.5)
      .attr(&quot;stroke&quot;, &quot;black&quot;)
      .attr(&quot;class&quot;, function(d) { console.log(d); return d;})
      .attr(&quot;fill-opacity&quot;, 0.5)
      .attr(&quot;stroke-opacity&quot;, 1)
      .on(&quot;mousedown&quot;, button_press);

menu_g.selectAll(&quot;text&quot;)
      .data(buttons)
      .enter()
      .append(&quot;text&quot;)
      .attr(&quot;x&quot;, function(d,i) { return width/4 * (i + 0.25) + 18;} )
      .attr(&quot;y&quot;, height - 14)
      .text(function(d) { return d; })
      .attr(&quot;text-anchor&quot;, &quot;start&quot;)
      .attr(&quot;font-family&quot;, &quot;Helvetica Neue&quot;)
      .attr(&quot;font-size&quot;, 15)
      .attr(&quot;font-weight&quot;, 200)
      .attr(&quot;fill&quot;, &quot;white&quot;)
      .attr(&quot;fill-opacity&quot;, 0.8);

function button_press() {
    var type = d3.select(this).attr(&quot;class&quot;)
    if (draw_bool[type]) {
        d3.select(this).attr(&quot;fill-opacity&quot;, 0);
        draw_bool[type] = false;
    } else {
        d3.select(this).attr(&quot;fill-opacity&quot;, 0.5)
        draw_bool[type] = true;
    }
}

// Set up optimization/gradient descent functions.
// SGD, Momentum, RMSProp, Adam.

function get_sgd_path(x0, y0, learning_rate, num_steps) {
    var sgd_history = [{&quot;x&quot;: scale_x.invert(x0), &quot;y&quot;: scale_y.invert(y0)}];
    var x1, y1, gradient;
    for (i = 0; i &lt; num_steps; i++) {
        gradient = grad_f(x0, y0);
        x1 = x0 - learning_rate * gradient[0]
        y1 = y0 - learning_rate * gradient[1]
        sgd_history.push({&quot;x&quot; : scale_x.invert(x1), &quot;y&quot; : scale_y.invert(y1)})
        x0 = x1
        y0 = y1
    }
    return sgd_history;
}

function get_momentum_path(x0, y0, learning_rate, num_steps, momentum) {
    var v_x = 0,
        v_y = 0;
    var momentum_history = [{&quot;x&quot;: scale_x.invert(x0), &quot;y&quot;: scale_y.invert(y0)}];
    var x1, y1, gradient;
    for (i=0; i &lt; num_steps; i++) {
        gradient = grad_f(x0, y0)
        v_x = momentum * v_x - learning_rate * gradient[0]
        v_y = momentum * v_y - learning_rate * gradient[1]
        x1 = x0 + v_x
        y1 = y0 + v_y
        momentum_history.push({&quot;x&quot; : scale_x.invert(x1), &quot;y&quot; : scale_y.invert(y1)})
        x0 = x1
        y0 = y1
    }
    return momentum_history
}

function get_rmsprop_path(x0, y0, learning_rate, num_steps, decay_rate, eps) {
    var cache_x = 0,
        cache_y = 0;
    var rmsprop_history = [{&quot;x&quot;: scale_x.invert(x0), &quot;y&quot;: scale_y.invert(y0)}];
    var x1, y1, gradient;
    for (i = 0; i &lt; num_steps; i++) {
        gradient = grad_f(x0, y0)
        cache_x = decay_rate * cache_x + (1 - decay_rate) * gradient[0] * gradient[0]
        cache_y = decay_rate * cache_y + (1 - decay_rate) * gradient[1] * gradient[1]
        x1 = x0 - learning_rate * gradient[0] / (Math.sqrt(cache_x) + eps)
        y1 = y0 - learning_rate * gradient[1] / (Math.sqrt(cache_y) + eps)
        rmsprop_history.push({&quot;x&quot; : scale_x.invert(x1), &quot;y&quot; : scale_y.invert(y1)})
        x0 = x1
        y0 = y1
    }
    return rmsprop_history;
}

function get_adam_path(x0, y0, learning_rate, num_steps, beta_1, beta_2, eps) {
    var m_x = 0,
        m_y = 0,
        v_x = 0,
        v_y = 0;
    var adam_history = [{&quot;x&quot;: scale_x.invert(x0), &quot;y&quot;: scale_y.invert(y0)}];
    var x1, y1, gradient;
    for (i = 0; i &lt; num_steps; i++) {
        gradient = grad_f(x0, y0)
        m_x = beta_1 * m_x + (1 - beta_1) * gradient[0]
        m_y = beta_1 * m_y + (1 - beta_1) * gradient[1]
        v_x = beta_2 * v_x + (1 - beta_2) * gradient[0] * gradient[0]
        v_y = beta_2 * v_y + (1 - beta_2) * gradient[1] * gradient[1]
        x1 = x0 - learning_rate * m_x / (Math.sqrt(v_x) + eps)
        y1 = y0 - learning_rate * m_y / (Math.sqrt(v_y) + eps)
        adam_history.push({&quot;x&quot; : scale_x.invert(x1), &quot;y&quot; : scale_y.invert(y1)})
        x0 = x1
        y0 = y1
    }
    return adam_history;
}

// Functions necessary for path visualizations

var line_function = d3.line()
                      .x(function(d) { return d.x; })
                      .y(function(d) { return d.y; });

function draw_path(path_data, type) {
    var gradient_path = gradient_path_g.selectAll(type)
                        .data(path_data)
                        .enter()
                        .append(&quot;path&quot;)
                        .attr(&quot;d&quot;, line_function(path_data.slice(0,1)))
                        .attr(&quot;class&quot;, type)
                        .attr(&quot;stroke-width&quot;, 3)
                        .attr(&quot;fill&quot;, &quot;none&quot;)
                        .attr(&quot;stroke-opacity&quot;, 0.5)
                        .transition()
                        .duration(drawing_time)
                        .delay(function(d,i) { return drawing_time * i; })
                        .attr(&quot;d&quot;, function(d,i) { return line_function(path_data.slice(0,i+1));})
                        .remove();

    gradient_path_g.append(&quot;path&quot;)
                   .attr(&quot;d&quot;, line_function(path_data))
                   .attr(&quot;class&quot;, type)
                   .attr(&quot;stroke-width&quot;, 3)
                   .attr(&quot;fill&quot;, &quot;none&quot;)
                   .attr(&quot;stroke-opacity&quot;, 0.5)
                   .attr(&quot;stroke-opacity&quot;, 0)
                   .transition()
                   .duration(path_data.length * drawing_time)
                   .attr(&quot;stroke-opacity&quot;, 0.5);
}

// Start minimization from click on contour map

function mousedown() {
    // Get initial point
    var point = d3.mouse(this);
    // Minimize and draw paths
    minimize(scale_x(point[0]), scale_y(point[1]));
}

function minimize(x0,y0) {
    gradient_path_g.selectAll(&quot;path&quot;).remove();

    if (draw_bool.SGD) {
        var sgd_data = get_sgd_path(x0, y0, 2e-2, 500);
        draw_path(sgd_data, &quot;sgd&quot;);
    }
    if (draw_bool.Momentum) {
        var momentum_data = get_momentum_path(x0, y0, 1e-2, 200, 0.8);
        draw_path(momentum_data, &quot;momentum&quot;);
    }
    if (draw_bool.RMSProp) {
        var rmsprop_data = get_rmsprop_path(x0, y0, 1e-2, 300, 0.99, 1e-6);
        draw_path(rmsprop_data, &quot;rmsprop&quot;);
    }
    if (draw_bool.Adam) {
        var adam_data = get_adam_path(x0, y0, 1e-2, 100, 0.7, 0.999, 1e-6);
        draw_path(adam_data, &quot;adam&quot;);
    }
}

// Start some minimization before click (to demonstrate how it works)
// minimize(scale_x(100), scale_y(20));
&lt;/script&gt;</content><author><name>ChenJin</name></author><summary type="html">Optimization on non convex functions in high dimensional spaces, like those encountered in deep learning, can be hard to visualize. However, we can learn a lot from visualizing optimization paths on simple 2d non convex functions. Click anywhere on the function contour to start a minimization. You can toggle the different algorithms by clicking the circles in the lower bar. The code is available here. Observations The above function is given by \[f(x, y) = x^2 + y^2 - a e^{-\frac{(x - 1)^2 + y^2}{c}} - b e^{-\frac{(x + 1)^2 + y^2}{d}}\] It is basically a quadratic “bowl” with two gaussians creating minima at (1, 0) and (-1, 0) respectively. The size of these minima is controlled by the \(a\) and \(b\) parameters. Different minima Starting from the same point, different algorithms will converge to different minima. Often, SGD and SGD with momentum will converge to the poorer minimum (the one on the right) while RMSProp and Adam will converge to the global minimum. For this particular function, Adam is the algorithm that converges to the global minimum from the most initializations. Only Adam (in green) converges to the global minimum. The effects of momentum Augmenting SGD with momentum has many advantages and often works better than the other standard algorithms for an appropriately chosen learning rate (check out this paper for more details). However, with the wrong learning rate, SGD with momentum can overshoot minima and this often leads to a spiraling pattern around the minimum. SGD with momentum spiraling towards the minimum. Standard SGD does not get you far SGD without momentum consistently performs the worst. The learning rate for SGD on the visualization is set to be artificially high (an order of magnitude higher than the other algorithms) in order for the optimization to converge in a reasonable amount of time. Classic optimization test functions There are many famous test functions for optimization which are useful for testing convergence, precision, robustness and performance of optimization algorithms. They also exhibit interesting behaviour which does not appear in the above function. Rastrigin The visualization for this function can be found here A Rastrigin function is a quadratic bowl overlayed with a grid of sine bumps creating a large number of local minima. SGD with momentum reaches the global optimum while all other algorithms get stuck in the same local minimum. In this example, SGD with momentum outperforms all other algorithms using the default parameter settings. The speed built up from the momentum allows it to power through the sine bumps and converge to the global minimum when other algorithms don’t. Of course, this would not necessarily be the case if the sine bumps had been scaled or spaced differently. Indeed, on the first function in this post, Adam performed the best while SGD with momentum performs the best on the Rastrigin function. This shows that there is no single algorithm that will perform the best on all functions, even in simple 2D cases. Rosenbrock The visualization for this function can be found here The Rosenbrock function has a single global minimum inside a parabolic shaped valley. Most algorithms rapidly converge to this valley, but it is typically difficult to converge to the global minimum within this valley. All algorithms find the global minimum but through very different paths While all algorithms converge to the optimum, the adaptive and non adaptive optimization algorithms approach the minimum through different paths. In higher dimensional problems, like in deep learning, different optimization algorithms will likely explore very different areas of parameter space. Conclusion Optimization algorithms can exhibit interesting behaviour, even on simple 2d functions. Of course, there are also many phenomena which we cannot hope to visualize on simple 2d problems. Understanding and visualizing optimization in deep learning in general is an active area of research. New optimization algorithms, like Eve or YellowFin, are also being developed. It would be interesting to modify the above code to visualize these more recent algorithms, although it is unclear whether they would differ significantly from momentum SGD on these toy problems.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="http://localhost:4000/assets/default-social-image.png" /><media:content medium="image" url="http://localhost:4000/assets/default-social-image.png" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>